# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gGmoTT_-8-_YLRRolAqAOfA_wXK8bmiA

# Proyek Deteksi Diabetes Menggunakan Dataset Pima Indians Diabetes

- Nama    : Rizaki Akbar
- email   : rizakiakbar004@gmail.com
- Username: rizaki_akbar_r0wz

# Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_score, recall_score, f1_score
from sklearn.feature_selection import RFE
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import precision_recall_curve, f1_score
from imblearn.over_sampling import SMOTE

"""#  Data Understanding & EDA"""

# Load Data
df = pd.read_csv('https://raw.githubusercontent.com/zaki903/submission-deteksi-diabetes/refs/heads/main/data/diabetes.csv')

# Melihat 5 baris pertama data
df.head()

# Cek Jumlah baris dan kolom
print("Ukuran data:", df.shape)

"""ukuran data terdiri dari 768 baris dan 9 kolom."""

# Cek nama kolom dan tipe data
print(df.info())

"""terdapat 9 fitur yang ada pada data diabetes, diantaranya adalah bertipe data int64 dan 2 fitur bertipe data float64 dan setiap fitur memiliki data sebanyak 768 data"""

# Statistik deskriptif dasar
print(df.describe())

"""pada fitur Skiinthicness BloodPressure, Glucose, BMI, dan Insulin memiliki nilai 0 sehingga ini tidak wajar secara medis karena secara medis nilai ini tidak boleh 0"""

# Cek Kolom medis yang tidak mungkin bernilai 0
cols_with_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# Cek berapa banyak nilai 0 di tiap kolom medis
for col in cols_with_zero:
    print(f"{col} memiliki {df[df[col] == 0].shape[0]} nilai 0")

"""ternyata banyak kesalahan nilai nol yang terjadi pada data.

# Visualisasi Data
"""

# Histplot dari semua fitur
df.hist(bins=20, figsize=(15, 10), color='skyblue')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Matriks Korelasi Fitur')
plt.show()

# deteksi outlier dengan IQR untuk Glucose
Q1 = df['Glucose'].quantile(0.25)
Q3 = df['Glucose'].quantile(0.75)
IQR = Q3 - Q1

outliers = df[(df['Glucose'] < Q1 - 1.5*IQR) | (df['Glucose'] > Q3 + 1.5*IQR)]
print("Jumlah outlier di Glucose:", outliers.shape[0])

"""#Data Preprocessing (Persiapan Data)

"""

# Salin DataFrame agar tidak merusak data asli
data = df.copy()

# Kolom yang tidak boleh memiliki 0
invalid_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# Ganti 0 dengan NaN
for col in invalid_cols:
    data[col] = data[col].replace(0, np.nan)

# Cek berapa banyak missing
print(data.isnull().sum())

"""pertama kita disini mengganti nilai 0 ke nilai NaN"""

# Imputasi dengan median (lebih stabil terhadap outlier)
for col in invalid_cols:
    median_value = data[col].median()
    data[col] = data[col].fillna(median_value)

"""lalu kita ganti nilai missing tadi(NaN) dengan nilai yang sering muncul atau median karena dengan nilai ini dia akan tahan terhadap outlier"""

# Feature Scaling / Normalisasi
X = data.drop('Outcome', axis=1)
y = data['Outcome']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""menggunakan StandardScaler untuk memastikan semua fitur berada pada skala yang sama, terutama penting untuk algoritma seperti KNN, SVM, dan Neural Network."""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

"""Memisahkan data menjadi data pelatihan dan data pengujian."""

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Distribusi kelas sebelum SMOTE:", y_train.value_counts())
print("Distribusi kelas setelah SMOTE:", y_train_resampled.value_counts())

"""menggunakan SMOTE agar model tidak model tidak bias terhadap kelas mayoritas."""

# Buat kembali DataFrame dari hasil scaling
scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Statistik deskriptif setelah scaling
print(scaled_df.describe())

model = LogisticRegression()
rfe = RFE(model, n_features_to_select=6)
fit = rfe.fit(X_train_resampled, y_train_resampled)

selected_features = X.columns[fit.support_]
print("Fitur yang dipilih:", selected_features)

"""Menggunakan fetaru selection agar akurasinya lebih maksimal

# Model Training dan Evaluasi Awal
"""

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {acc:.4f}")

    return acc

lr = LogisticRegression(
    max_iter=1000,
    random_state=42
)

lr.fit(X_train_resampled, y_train_resampled)
print("\n--- Logistic Regression ---")
evaluate_model(lr, X_test, y_test)

rf = RandomForestClassifier(
    n_estimators=180,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    bootstrap=True,
    random_state=42
)

rf.fit(X_train_resampled, y_train_resampled)
print("\n--- Random Forest ---")
evaluate_model(rf, X_test, y_test)

svm = SVC(
    probability=True,
    random_state=42
)

svm.fit(X_train_resampled, y_train_resampled)
print("\n--- Support Vector Machine ---")
evaluate_model(svm, X_test, y_test)

knn = KNeighborsClassifier(
    n_neighbors=5,
    weights='uniform',
    algorithm='auto',
    leaf_size=30,
    p=2,
    metric='minkowski'
)

knn.fit(X_train_resampled, y_train_resampled)
print("\n--- K-Nearest Neighbors ---")
evaluate_model(knn, X_test, y_test)

"""### Evaluate Model

| Model                  | Accuracy   | Precision (Class 1) | Recall (Class 1) | F1-Score (Class 1) |
| ---------------------- | ---------- | ------------------- | ---------------- | ------------------ |
| Logistic Regression    | 72.08%     | 0.59                | 0.69             | 0.63               |
| Random Forest          | **75.32%** | **0.64**            | **0.67**         | **0.65**           |
| Support Vector Machine | 72.73%     | 0.60                | 0.69             | 0.64               |
| K-Nearest Neighbors    | 69.48%     | 0.55                | **0.72**         | 0.62               |


### Inter Pretasi Hasil
1. Random Forest Menjadi Model Terbaik Sementara
- Akurasi tertinggi (75.32%)
- Precision dan F1-score untuk penderita diabetes (Outcome = 1) juga tertinggi
- Hasil ini menunjukkan keseimbangan prediksi untuk kedua kelas dan potensi terbaik sebagai baseline model

2. Logistic Regression dan SVM Stabil, Tapi Kurang dalam Precision
- Recall tinggi (baik untuk mendeteksi penderita), tapi precision rendah, lebih banyak false positives.

3. K-Nearest Neighbors Paling Lemah
- Walau recall tinggi (0.72), precision-nya paling rendah (0.55) â†’ model sering salah deteksi orang sehat sebagai sakit.
- Performa keseluruhan (F1 dan akurasi) juga paling rendah.

### Kesimpulan
kita akan memilihih random forest untuk model utama kita dan kita akan melukukan hyper parameter tuning agar meningkatkan performa model.

# Hyper Paramter Tuning

Percobaan pertama tuning menggunakan random forest tuning menggunakan grid paramenter
"""

# Random Forest tuning
rf = RandomForestClassifier(random_state=42)

param_grid_rf = {
    'n_estimators': [100, 200, 300],       # jumlah pohon
    'max_depth': [None, 10, 20, 30],       # kedalaman pohon
    'min_samples_split': [2, 5, 10],       # minimum sampel untuk split
    'min_samples_leaf': [1, 2, 4],         # minimum sampel pada daun
    'bootstrap': [True, False]              # sampling bootstrap atau tidak
}

grid_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf,
                       cv=5, scoring='f1', n_jobs=-1, verbose=2)

grid_rf.fit(X_train_resampled, y_train_resampled)

print("Best RF params:", grid_rf.best_params_)
print("Best RF F1 score:", grid_rf.best_score_)

best_rf = grid_rf.best_estimator_
print("\nEvaluasi Random Forest terbaik:")
evaluate_model(best_rf, X_test, y_test)

"""#### Hyperparameter Tuning dengan RandomizedSearchCV"""

rf = RandomForestClassifier(random_state=42)

# Definisikan distribusi hyperparameter
param_dist_rf = {
    'n_estimators': randint(100, 500),
    'max_depth': [None] + list(range(10, 50, 10)),
    'min_samples_split': randint(2, 11),
    'min_samples_leaf': randint(1, 5),
    'bootstrap': [True, False]
}

random_search_rf = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist_rf,
    n_iter=50,              # jumlah kombinasi yang dicoba
    scoring='f1',
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

random_search_rf.fit(X_train_resampled, y_train_resampled)

print("Best RF params:", random_search_rf.best_params_)
print("Best RF F1 score:", random_search_rf.best_score_)

best_rf = random_search_rf.best_estimator_
print("\nEvaluasi Random Forest terbaik:")
evaluate_model(best_rf, X_test, y_test)

"""### Threshold Tuning"""

# Dapatkan probabilitas prediksi kelas positif (diabetes)
y_proba = best_rf.predict_proba(X_test)[:, 1]

# Cek berbagai threshold untuk menemukan yang optimal berdasarkan f1-score
thresholds = np.arange(0, 1, 0.01)
f1_scores = []

for thresh in thresholds:
    y_pred_thresh = (y_proba >= thresh).astype(int)
    f1 = f1_score(y_test, y_pred_thresh)
    f1_scores.append(f1)

best_thresh = thresholds[np.argmax(f1_scores)]
print(f"Best threshold based on F1 score: {best_thresh:.2f}")

# Prediksi ulang dengan threshold terbaik
y_pred_best_thresh = (y_proba >= best_thresh).astype(int)

# Evaluasi ulang
from sklearn.metrics import classification_report, confusion_matrix

print("Confusion Matrix with tuned threshold:")
print(confusion_matrix(y_test, y_pred_best_thresh))
print("\nClassification Report with tuned threshold:")
print(classification_report(y_test, y_pred_best_thresh))

"""### Voting Classifier"""

# Inisialisasi model, pakai parameter terbaik dari tuning sebelumnya jika ada
log_reg = LogisticRegression(random_state=42, max_iter=1000)
svm_clf = SVC(probability=True, random_state=42)
rf_clf = best_rf  # model Random Forest hasil tuning

voting_clf = VotingClassifier(
    estimators=[('lr', log_reg), ('svm', svm_clf), ('rf', rf_clf)],
    voting='soft'  # voting berdasarkan probabilitas
)

# Latih ensemble di data train (yang sudah di-resample)
voting_clf.fit(X_train_resampled, y_train_resampled)

# Evaluasi di test set
y_pred_vote = voting_clf.predict(X_test)

print("Confusion Matrix Voting Classifier:")
print(confusion_matrix(y_test, y_pred_vote))
print("\nClassification Report Voting Classifier:")
print(classification_report(y_test, y_pred_vote))

"""### Visualisasi Hasil"""

def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

def get_metrics(y_true, y_pred):
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred)
    }

# 1. Prediksi awal (threshold 0.5)
y_pred_base = best_rf.predict(X_test)
metrics_base = get_metrics(y_test, y_pred_base)
cm_base = confusion_matrix(y_test, y_pred_base)

# 2. Prediksi threshold tuning
y_proba = best_rf.predict_proba(X_test)[:, 1]
y_pred_thresh = (y_proba >= best_thresh).astype(int)
metrics_thresh = get_metrics(y_test, y_pred_thresh)
cm_thresh = confusion_matrix(y_test, y_pred_thresh)

# 3. Prediksi voting classifier
y_pred_vote = voting_clf.predict(X_test)
metrics_vote = get_metrics(y_test, y_pred_vote)
cm_vote = confusion_matrix(y_test, y_pred_vote)

# Plot confusion matrix
plot_confusion_matrix(cm_base, 'Confusion Matrix - Base RF (threshold=0.5)')
plot_confusion_matrix(cm_thresh, f'Confusion Matrix - RF with Threshold {best_thresh:.2f}')
plot_confusion_matrix(cm_vote, 'Confusion Matrix - Voting Classifier')

# Plot perbandingan metrik
labels = ['Base RF', f'Threshold {best_thresh:.2f}', 'Voting Classifier']
accuracy_vals = [metrics_base['accuracy'], metrics_thresh['accuracy'], metrics_vote['accuracy']]
precision_vals = [metrics_base['precision'], metrics_thresh['precision'], metrics_vote['precision']]
recall_vals = [metrics_base['recall'], metrics_thresh['recall'], metrics_vote['recall']]
f1_vals = [metrics_base['f1'], metrics_thresh['f1'], metrics_vote['f1']]

x = range(len(labels))
width = 0.2

plt.figure(figsize=(10,6))
plt.bar([p - width*1.5 for p in x], accuracy_vals, width=width, label='Accuracy')
plt.bar([p - width*0.5 for p in x], precision_vals, width=width, label='Precision')
plt.bar([p + width*0.5 for p in x], recall_vals, width=width, label='Recall')
plt.bar([p + width*1.5 for p in x], f1_vals, width=width, label='F1 Score')

plt.xticks(x, labels)
plt.ylim(0,1)
plt.title('Comparison of Metrics Across Models')
plt.legend()
plt.show()